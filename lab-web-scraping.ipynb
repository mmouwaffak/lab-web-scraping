{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "7e7a1ab8-2599-417d-9a65-25ef07f3a786",
      "metadata": {
        "id": "7e7a1ab8-2599-417d-9a65-25ef07f3a786"
      },
      "source": [
        "# Lab | Web Scraping"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "ce8882fc-4815-4567-92fa-b4816358ba7d",
      "metadata": {
        "id": "ce8882fc-4815-4567-92fa-b4816358ba7d"
      },
      "source": [
        "Welcome to the \"Books to Scrape\" Web Scraping Adventure Lab!\n",
        "\n",
        "**Objective**\n",
        "\n",
        "In this lab, we will embark on a mission to unearth valuable insights from the data available on Books to Scrape, an online platform showcasing a wide variety of books. As data analyst, you have been tasked with scraping a specific subset of book data from Books to Scrape to assist publishing companies in understanding the landscape of highly-rated books across different genres. Your insights will help shape future book marketing strategies and publishing decisions.\n",
        "\n",
        "**Background**\n",
        "\n",
        "In a world where data has become the new currency, businesses are leveraging big data to make informed decisions that drive success and profitability. The publishing industry, much like others, utilizes data analytics to understand market trends, reader preferences, and the performance of books based on factors such as genre, author, and ratings. Books to Scrape serves as a rich source of such data, offering detailed information about a diverse range of books, making it an ideal platform for extracting insights to aid in informed decision-making within the literary world.\n",
        "\n",
        "**Task**\n",
        "\n",
        "Your task is to create a Python script using BeautifulSoup and pandas to scrape Books to Scrape book data, focusing on book ratings and genres. The script should be able to filter books with ratings above a certain threshold and in specific genres. Additionally, the script should structure the scraped data in a tabular format using pandas for further analysis.\n",
        "\n",
        "**Expected Outcome**\n",
        "\n",
        "A function named `scrape_books` that takes two parameters: `min_rating` and `max_price`. The function should scrape book data from the \"Books to Scrape\" website and return a `pandas` DataFrame with the following columns:\n",
        "\n",
        "**Expected Outcome**\n",
        "\n",
        "- A function named `scrape_books` that takes two parameters: `min_rating` and `max_price`.\n",
        "- The function should return a DataFrame with the following columns:\n",
        "  - **UPC**: The Universal Product Code (UPC) of the book.\n",
        "  - **Title**: The title of the book.\n",
        "  - **Price (£)**: The price of the book in pounds.\n",
        "  - **Rating**: The rating of the book (1-5 stars).\n",
        "  - **Genre**: The genre of the book.\n",
        "  - **Availability**: Whether the book is in stock or not.\n",
        "  - **Description**: A brief description or product description of the book (if available).\n",
        "  \n",
        "You will execute this script to scrape data for books with a minimum rating of `4.0 and above` and a maximum price of `£20`. \n",
        "\n",
        "Remember to experiment with different ratings and prices to ensure your code is versatile and can handle various searches effectively!\n",
        "\n",
        "**Resources**\n",
        "\n",
        "- [Beautiful Soup Documentation](https://www.crummy.com/software/BeautifulSoup/bs4/doc/)\n",
        "- [Pandas Documentation](https://pandas.pydata.org/pandas-docs/stable/index.html)\n",
        "- [Books to Scrape](https://books.toscrape.com/)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "3519921d-5890-445b-9a33-934ed8ee378c",
      "metadata": {
        "id": "3519921d-5890-445b-9a33-934ed8ee378c"
      },
      "source": [
        "**Hint**\n",
        "\n",
        "Your first mission is to familiarize yourself with the **Books to Scrape** website. Navigate to [Books to Scrape](http://books.toscrape.com/) and explore the available books to understand their layout and structure. \n",
        "\n",
        "Next, think about how you can set parameters for your data extraction:\n",
        "\n",
        "- **Minimum Rating**: Focus on books with a rating of 4.0 and above.\n",
        "- **Maximum Price**: Filter for books priced up to £20.\n",
        "\n",
        "After reviewing the site, you can construct a plan for scraping relevant data. Pay attention to the details displayed for each book, including the title, price, rating, and availability. This will help you identify the correct HTML elements to target with your scraping script.\n",
        "\n",
        "Make sure to build your scraping URL and logic based on the patterns you observe in the HTML structure of the book listings!"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "25a83a0d-a742-49f6-985e-e27887cbf922",
      "metadata": {
        "id": "25a83a0d-a742-49f6-985e-e27887cbf922"
      },
      "source": [
        "\n",
        "---\n",
        "\n",
        "**Best of luck! Immerse yourself in the world of books, and may the data be with you!**"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "7b75cf0d-9afa-4eec-a9e2-befeac68b2a0",
      "metadata": {
        "id": "7b75cf0d-9afa-4eec-a9e2-befeac68b2a0"
      },
      "source": [
        "**Important Note**:\n",
        "\n",
        "In the fast-changing online world, websites often update and change their structures. When you try this lab, the **Books to Scrape** website might differ from what you expect.\n",
        "\n",
        "If you encounter issues due to these changes, like new rules or obstacles preventing data extraction, don’t worry! Get creative.\n",
        "\n",
        "You can choose another website that interests you and is suitable for scraping data. Options like Wikipedia, The New York Times, or even library databases are great alternatives. The main goal remains the same: extract useful data and enhance your web scraping skills while exploring a source of information you enjoy. This is your opportunity to practice and adapt to different web environments!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "40359eee-9cd7-4884-bfa4-83344c222305",
      "metadata": {
        "id": "40359eee-9cd7-4884-bfa4-83344c222305"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: beautifulsoup4 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (4.12.3)\n",
            "Requirement already satisfied: requests in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (2.32.3)\n",
            "Requirement already satisfied: pandas in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (2.3.3)\n",
            "Requirement already satisfied: soupsieve>1.2 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from beautifulsoup4) (2.5)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from requests) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from requests) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from requests) (2.3.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from requests) (2025.11.12)\n",
            "Requirement already satisfied: numpy>=1.26.0 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from pandas) (2.3.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from pandas) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from pandas) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in c:\\users\\mmouw\\anaconda3\\lib\\site-packages (from python-dateutil>=2.8.2->pandas) (1.17.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install beautifulsoup4 requests pandas"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "7c289394",
      "metadata": {},
      "outputs": [],
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import pandas as pd\n",
        "import time"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "9c14bbcf",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Found 50 categories to scrape\n",
            "\n",
            "Scraping category: Travel\n",
            "  - Page 1: Found 11 books\n",
            "Scraping category: Mystery\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 12 books\n",
            "Scraping category: Historical Fiction\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 6 books\n",
            "Scraping category: Sequential Art\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 20 books\n",
            "  - Page 3: Found 20 books\n",
            "  - Page 4: Found 15 books\n",
            "Scraping category: Classics\n",
            "  - Page 1: Found 19 books\n",
            "Scraping category: Philosophy\n",
            "  - Page 1: Found 11 books\n",
            "Scraping category: Romance\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 15 books\n",
            "Scraping category: Womens Fiction\n",
            "  - Page 1: Found 17 books\n",
            "Scraping category: Fiction\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 20 books\n",
            "  - Page 3: Found 20 books\n",
            "  - Page 4: Found 5 books\n",
            "Scraping category: Childrens\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 9 books\n",
            "Scraping category: Religion\n",
            "  - Page 1: Found 7 books\n",
            "Scraping category: Nonfiction\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 20 books\n",
            "  - Page 3: Found 20 books\n",
            "  - Page 4: Found 20 books\n",
            "  - Page 5: Found 20 books\n",
            "  - Page 6: Found 10 books\n",
            "Scraping category: Music\n",
            "  - Page 1: Found 13 books\n",
            "Scraping category: Default\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 20 books\n",
            "  - Page 3: Found 20 books\n",
            "  - Page 4: Found 20 books\n",
            "  - Page 5: Found 20 books\n",
            "  - Page 6: Found 20 books\n",
            "  - Page 7: Found 20 books\n",
            "  - Page 8: Found 12 books\n",
            "Scraping category: Science Fiction\n",
            "  - Page 1: Found 16 books\n",
            "Scraping category: Sports and Games\n",
            "  - Page 1: Found 5 books\n",
            "Scraping category: Add a comment\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 20 books\n",
            "  - Page 3: Found 20 books\n",
            "  - Page 4: Found 7 books\n",
            "Scraping category: Fantasy\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 20 books\n",
            "  - Page 3: Found 8 books\n",
            "Scraping category: New Adult\n",
            "  - Page 1: Found 6 books\n",
            "Scraping category: Young Adult\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 20 books\n",
            "  - Page 3: Found 14 books\n",
            "Scraping category: Science\n",
            "  - Page 1: Found 14 books\n",
            "Scraping category: Poetry\n",
            "  - Page 1: Found 19 books\n",
            "Scraping category: Paranormal\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Art\n",
            "  - Page 1: Found 8 books\n",
            "Scraping category: Psychology\n",
            "  - Page 1: Found 7 books\n",
            "Scraping category: Autobiography\n",
            "  - Page 1: Found 9 books\n",
            "Scraping category: Parenting\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Adult Fiction\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Humor\n",
            "  - Page 1: Found 10 books\n",
            "Scraping category: Horror\n",
            "  - Page 1: Found 17 books\n",
            "Scraping category: History\n",
            "  - Page 1: Found 18 books\n",
            "Scraping category: Food and Drink\n",
            "  - Page 1: Found 20 books\n",
            "  - Page 2: Found 10 books\n",
            "Scraping category: Christian Fiction\n",
            "  - Page 1: Found 6 books\n",
            "Scraping category: Business\n",
            "  - Page 1: Found 12 books\n",
            "Scraping category: Biography\n",
            "  - Page 1: Found 5 books\n",
            "Scraping category: Thriller\n",
            "  - Page 1: Found 11 books\n",
            "Scraping category: Contemporary\n",
            "  - Page 1: Found 3 books\n",
            "Scraping category: Spirituality\n",
            "  - Page 1: Found 6 books\n",
            "Scraping category: Academic\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Self Help\n",
            "  - Page 1: Found 5 books\n",
            "Scraping category: Historical\n",
            "  - Page 1: Found 2 books\n",
            "Scraping category: Christian\n",
            "  - Page 1: Found 3 books\n",
            "Scraping category: Suspense\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Short Stories\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Novels\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Health\n",
            "  - Page 1: Found 4 books\n",
            "Scraping category: Politics\n",
            "  - Page 1: Found 3 books\n",
            "Scraping category: Cultural\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Erotica\n",
            "  - Page 1: Found 1 books\n",
            "Scraping category: Crime\n",
            "  - Page 1: Found 1 books\n",
            "\n",
            "Total books found: 75\n",
            "                UPC                                              Title  \\\n",
            "0  4280ac3eab57aa5d                                  The Girl You Lost   \n",
            "1  19fec36a1dfb4c16  A Spy's Devotion (The Regency Spies of London #1)   \n",
            "2  51653ef291ab7ddc                                    This One Summer   \n",
            "3  0fa6dceead7ce47a  Princess Jellyfish 2-in-1 Omnibus, Vol. 01 (Pr...   \n",
            "4  889139b8e9c4cb36  Outcast, Vol. 1: A Darkness Surrounds Him (Out...   \n",
            "5  7b599c8bfcdd8d30                                      Camp Midnight   \n",
            "6  47271d8e08b5d31c           Batman: The Dark Knight Returns (Batman)   \n",
            "7  ac8e3949d284e9a9  Superman Vol. 1: Before Truth (Superman by Gen...   \n",
            "8  aeb51dfbe8aeec59                                        Roller Girl   \n",
            "9  c0db71f6bb14db11           Fruits Basket, Vol. 2 (Fruits Basket #2)   \n",
            "\n",
            "   Price (£)  Rating               Genre Availability  \\\n",
            "0      12.29       5             Mystery     In stock   \n",
            "1      16.97       5  Historical Fiction     In stock   \n",
            "2      19.49       4      Sequential Art     In stock   \n",
            "3      13.61       5      Sequential Art     In stock   \n",
            "4      15.44       4      Sequential Art     In stock   \n",
            "5      17.08       4      Sequential Art     In stock   \n",
            "6      15.38       5      Sequential Art     In stock   \n",
            "7      11.89       5      Sequential Art     In stock   \n",
            "8      14.10       5      Sequential Art     In stock   \n",
            "9      11.64       5      Sequential Art     In stock   \n",
            "\n",
            "                                         Description  \n",
            "0  Eighteen years ago your baby daughter was snat...  \n",
            "1  In England’s Regency era, manners and elegance...  \n",
            "2  Every summer, Rose goes with her mom and dad t...  \n",
            "3  THE LONG-AWAITED STORY OF FANGIRLS TAKING ON T...  \n",
            "4  NEW HORROR SERIES FROM THE WALKING DEAD CREATO...  \n",
            "5  Ben 10 and Big Hero 6 creator Steven T. Seagle...  \n",
            "6  This masterpiece of modern comics storytelling...  \n",
            "7  Superman is going through some changes. First,...  \n",
            "8  For fans of Raina Telgemeier’s Smile, a heartw...  \n",
            "9  A family with an ancient curse...And the girl ...  \n",
            "\n",
            "Total books: 75\n",
            "Average price: £14.57\n",
            "\n",
            "Rating distribution:\n",
            "Rating\n",
            "4    33\n",
            "5    42\n",
            "Name: count, dtype: int64\n",
            "\n",
            "Top 5 genres:\n",
            "Genre\n",
            "Default           10\n",
            "Sequential Art     9\n",
            "Young Adult        7\n",
            "Nonfiction         6\n",
            "Fiction            5\n",
            "Name: count, dtype: int64\n"
          ]
        }
      ],
      "source": [
        "# After inspection of the website structure, we define a function to scrape book data\n",
        "\n",
        "\n",
        "def scrape_books(min_rating, max_price):\n",
        "    \n",
        "    rating_map = {'One': 1, 'Two': 2, 'Three': 3, 'Four': 4, 'Five': 5}\n",
        "    books_list = []\n",
        "    \n",
        "    base_url = \"https://books.toscrape.com/\"\n",
        "    catalogue_url = base_url + \"catalogue/\"\n",
        "    \n",
        "    # Get main page to find all categories\n",
        "    response = requests.get(base_url)\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    \n",
        "    # Find all categories\n",
        "    category_list = soup.find('ul', class_='nav nav-list')\n",
        "    categories = category_list.find('ul').find_all('li')\n",
        "    \n",
        "    # Extract category URLs and names\n",
        "    category_urls = []\n",
        "    for category in categories:\n",
        "        link = category.find('a')\n",
        "        if link:\n",
        "            category_name = link.text.strip()\n",
        "            category_url = base_url + link['href']\n",
        "            category_urls.append((category_name, category_url))\n",
        "    \n",
        "    print(f\"Found {len(category_urls)} categories to scrape\\n\")\n",
        "\n",
        "    # Scrape each category\n",
        "    for category_name, category_url in category_urls:\n",
        "        print(f\"Scraping category: {category_name}\")\n",
        "        page_num = 1\n",
        "        \n",
        "        while True:\n",
        "            # Pagination\n",
        "            if page_num == 1:\n",
        "                url = category_url\n",
        "            else:\n",
        "                url = category_url.replace('index.html', f'page-{page_num}.html')\n",
        "            \n",
        "            try:\n",
        "                response = requests.get(url)\n",
        "                \n",
        "                # Stop if page doesn't exist\n",
        "                if response.status_code == 404:\n",
        "                    break\n",
        "                \n",
        "                soup = BeautifulSoup(response.content, 'html.parser')\n",
        "                books = soup.find_all('article', class_='product_pod')\n",
        "                \n",
        "                if not books:\n",
        "                    break\n",
        "                \n",
        "                # Process each book on the page\n",
        "                for book in books:\n",
        "                    # Extract title\n",
        "                    title = book.find('h3').find('a')['title']\n",
        "                    \n",
        "                    # Extract price\n",
        "                    price = float(book.find('p', class_='price_color').text.replace('£', ''))\n",
        "                    \n",
        "                    # Extract rating\n",
        "                    rating_class = book.find('p', class_='star-rating')['class'][1]\n",
        "                    rating = rating_map[rating_class]\n",
        "                    \n",
        "                    # Apply filters\n",
        "                    if rating >= min_rating and price <= max_price:\n",
        "                        availability_elem = book.find('p', class_='instock availability')\n",
        "                        availability = availability_elem.text.strip() if availability_elem else 'N/A'\n",
        "                        \n",
        "                        # Get book detail page URL\n",
        "                        book_link = book.find('h3').find('a')['href']\n",
        "                        book_link = book_link.replace('../../../', '')\n",
        "                        \n",
        "                        # Scrape individual book page for UPC and Description\n",
        "                        book_detail_url = catalogue_url + book_link\n",
        "                        detail_response = requests.get(book_detail_url)\n",
        "                        detail_soup = BeautifulSoup(detail_response.content, 'html.parser')\n",
        "                        \n",
        "                        # Extract UPC from product information table\n",
        "                        upc = None\n",
        "                        product_table = detail_soup.find('table', class_='table table-striped')\n",
        "                        if product_table:\n",
        "                            rows = product_table.find_all('tr')\n",
        "                            for row in rows:\n",
        "                                if 'UPC' in row.find('th').text:\n",
        "                                    upc = row.find('td').text.strip()\n",
        "                                    break\n",
        "                        \n",
        "                        # Extract description\n",
        "                        description = None\n",
        "                        description_div = detail_soup.find('div', id='product_description')\n",
        "                        if description_div:\n",
        "                            description_p = description_div.find_next_sibling('p')\n",
        "                            if description_p:\n",
        "                                description = description_p.text.strip()\n",
        "                        \n",
        "                        # Add book to list\n",
        "                        books_list.append({\n",
        "                            'UPC': upc,\n",
        "                            'Title': title,\n",
        "                            'Price (£)': price,\n",
        "                            'Rating': rating,\n",
        "                            'Genre': category_name,\n",
        "                            'Availability': availability,\n",
        "                            'Description': description\n",
        "                        })\n",
        "                        \n",
        "\n",
        "                \n",
        "                print(f\"  - Page {page_num}: Found {len(books)} books\")\n",
        "                page_num += 1\n",
        "                \n",
        "            except Exception as e:\n",
        "                print(f\"  - Error on page {page_num}: {e}\")\n",
        "                break\n",
        "    \n",
        "    print(f\"\\nTotal books found: {len(books_list)}\")\n",
        "    \n",
        "    return pd.DataFrame(books_list)\n",
        "\n",
        "\n",
        "# min_rating=4.0 et and max_price=20.0\n",
        "books_df = scrape_books(min_rating=4.0, max_price=20.0)\n",
        "\n",
        "\n",
        "print(books_df.head(10))\n",
        "\n",
        "# Statistiques\n",
        "print(f\"\\nTotal books: {len(books_df)}\")\n",
        "print(f\"Average price: £{books_df['Price (£)'].mean():.2f}\")\n",
        "print(f\"\\nRating distribution:\")\n",
        "print(books_df['Rating'].value_counts().sort_index())\n",
        "print(f\"\\nTop 5 genres:\")\n",
        "print(books_df['Genre'].value_counts().head())\n",
        "\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "base",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.13.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
